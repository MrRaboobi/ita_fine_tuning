{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl accelerate transformers datasets peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom trl import DPOTrainer, DPOConfig\nimport torch\n\n# Load full dataset\ndataset = load_dataset(\"PKU-Alignment/PKU-SafeRLHF\")\n\n# Optional: reduce size for Kaggle memory constraints\ntrain_subset = dataset[\"train\"].select(range(5000))\ntest_subset = dataset[\"test\"].select(range(500))\n\n# Convert to DPO format\ndef convert_to_dpo(example):\n    if example[\"better_response_id\"] == 0:\n        chosen = example[\"response_0\"]\n        rejected = example[\"response_1\"]\n    else:\n        chosen = example[\"response_1\"]\n        rejected = example[\"response_0\"]\n\n    return {\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": chosen,\n        \"rejected\": rejected\n    }\n\ndpo_train_dataset = train_subset.map(convert_to_dpo)\ndpo_test_dataset = test_subset.map(convert_to_dpo)\n\nprint(\"Train sample:\", dpo_train_dataset[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name_or_path = \"/kaggle/input/Model/SFT_best_model\"  \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\ntokenizer.pad_token = tokenizer.eos_token  \n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dpo_config = DPOConfig(\n    beta=0.30,\n    learning_rate=3e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    gradient_accumulation_steps=1,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=10,\n    output_dir=\"/kaggle/working/SFT_dpo_model\",  # Save model here\n    remove_unused_columns=False,\n    fp16=torch.cuda.is_available()\n)\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=dpo_config.output_dir,\n    per_device_train_batch_size=dpo_config.per_device_train_batch_size,\n    per_device_eval_batch_size=dpo_config.per_device_eval_batch_size,\n    gradient_accumulation_steps=dpo_config.gradient_accumulation_steps,\n    learning_rate=dpo_config.learning_rate,\n    num_train_epochs=dpo_config.num_train_epochs,\n    logging_dir=f\"{dpo_config.output_dir}/logs\",\n    logging_steps=dpo_config.logging_steps,\n    evaluation_strategy=\"steps\",\n    eval_steps=dpo_config.eval_steps,\n    save_strategy=\"steps\",\n    save_steps=dpo_config.save_steps,\n    save_total_limit=dpo_config.save_total_limit,\n    report_to=\"none\",  # prevent wandb init in Kaggle\n    bf16=torch.cuda.is_available(),  # use bf16 if available\n)\n\n# Initialize trainer\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=None,  \n    args=training_args,\n    train_dataset=dpo_train_dataset,\n    eval_dataset=dpo_test_dataset,\n    tokenizer=tokenizer,\n    dpo_config=dpo_config,\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save final model\ntrainer.model.save_pretrained(dpo_config.output_dir)\ntokenizer.save_pretrained(dpo_config.output_dir)\n\nprint(\"DPO fine-tuning complete. Model saved to:\", dpo_config.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport numpy as np\n\n# Load the DPO fine-tuned model and tokenizer\nmodel_path =  \"/kaggle/input/Model/SFT_best_model\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel.eval()\nif torch.cuda.is_available():\n    model.cuda()\n\n\ndataset = load_dataset(\"PKU-Alignment/PKU-SafeRLHF\")[\"test\"].select(range(500))\ndef convert_to_dpo(example):\n    if example[\"better_response_id\"] == 0:\n        chosen = example[\"response_0\"]\n        rejected = example[\"response_1\"]\n    else:\n        chosen = example[\"response_1\"]\n        rejected = example[\"response_0\"]\n    return {\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": chosen,\n        \"rejected\": rejected\n    }\n\neval_dataset = dataset.map(convert_to_dpo)\n\n\ndef compute_log_prob(prompt, response):\n    full_text = prompt + response\n    inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=512)\n    with torch.no_grad():\n        input_ids = inputs[\"input_ids\"].to(model.device)\n        attention_mask = inputs[\"attention_mask\"].to(model.device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits[:, :-1, :]\n        labels = input_ids[:, 1:]\n\n        # Only compute log probs for the response tokens\n        label_mask = (input_ids[0] != tokenizer.pad_token_id).cpu().numpy()\n        response_start = len(tokenizer(prompt)[\"input_ids\"])\n        label_mask[:response_start] = 0  # mask out prompt\n\n        # Cross-entropy loss per token (negative log-likelihood)\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        chosen_token_logprobs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n\n        masked_log_probs = chosen_token_logprobs[0][torch.tensor(label_mask[1:], dtype=torch.bool)]\n        total_log_prob = masked_log_probs.sum().item()\n        return total_log_prob\n\n# Evaluate accuracy over the dataset\ncorrect = 0\ntotal = len(eval_dataset)\n\nfor example in tqdm(eval_dataset, desc=\"Evaluating\"):\n    prompt = example[\"prompt\"]\n    logp_chosen = compute_log_prob(prompt, example[\"chosen\"])\n    logp_rejected = compute_log_prob(prompt, example[\"rejected\"])\n\n    if logp_chosen > logp_rejected:\n        correct += 1\n\naccuracy = correct / total\nprint(f\"\\nDPO Evaluation Accuracy: {accuracy * 100:.2f}% ({correct}/{total})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trial 1\n- **LoRA Rank:** 8  \n- **DPO Beta:** 0.10  \n- **Learning Rate:** 5e-6  \n- **Batch Size:** 2  \n- **Epochs:** 1  \n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=DPOConfig(\n        beta=0.10,\n        learning_rate=5e-6,\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        num_train_epochs=1,\n        gradient_accumulation_steps=4,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        remove_unused_columns=False,\n        fp16=torch.cuda.is_available()\n    ),\n    tokenizer=tokenizer,\n    train_dataset=dpo_train_dataset,\n    eval_dataset=dpo_test_dataset\n)\ntrainer.train()\n\n\n### Trial 2\n- **LoRA Rank:** 16  \n- **DPO Beta:** 0.20  \n- **Learning Rate:** 1e-5  \n- **Batch Size:** 4  \n- **Epochs:** 2  \n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=DPOConfig(\n        beta=0.20,\n        learning_rate=1e-5,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=2,\n        gradient_accumulation_steps=2,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        remove_unused_columns=False,\n        fp16=torch.cuda.is_available()\n    ),\n    tokenizer=tokenizer,\n    train_dataset=dpo_train_dataset,\n    eval_dataset=dpo_test_dataset\n)\ntrainer.train()\n\n### Trial 3\n- **LoRA Rank:** 8  \n- **DPO Beta:** 0.05  \n- **Learning Rate:** 2e-6  \n- **Batch Size:** 1  \n- **Epochs:** 1  \n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=DPOConfig(\n        beta=0.05,\n        learning_rate=2e-6,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        num_train_epochs=1,\n        gradient_accumulation_steps=8,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        remove_unused_columns=False,\n        fp16=torch.cuda.is_available()\n    ),\n    tokenizer=tokenizer,\n    train_dataset=dpo_train_dataset,\n    eval_dataset=dpo_test_dataset\n)\ntrainer.train()\n\n### Trial 4 - Best Accuracy\n- **LoRA Rank:** 16  \n- **DPO Beta:** 0.30  \n- **Learning Rate:** 3e-5  \n- **Batch Size:** 4  \n- **Epochs:** 3  \n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=DPOConfig(\n        beta=0.30,\n        learning_rate=3e-5,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=3,\n        gradient_accumulation_steps=1,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        remove_unused_columns=False,\n        fp16=torch.cuda.is_available()\n    ),\n    tokenizer=tokenizer,\n    train_dataset=dpo_train_dataset,\n    eval_dataset=dpo_test_dataset\n)\ntrainer.train()\n\n### Trial 5\n- **LoRA Rank:** 8  \n- **DPO Beta:** 0.15  \n- **Learning Rate:** 7e-6  \n- **Batch Size:** 2  \n- **Epochs:** 2  \n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    args=DPOConfig(\n        beta=0.15,\n        learning_rate=7e-6,\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        num_train_epochs=2,\n        gradient_accumulation_steps=4,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        remove_unused_columns=False,\n        fp16=torch.cuda.is_available()\n    ),\n    tokenizer=tokenizer,\n    train_dataset=dpo_train_dataset,\n    eval_dataset=dpo_test_dataset\n)\ntrainer.train()\n","metadata":{}}]}